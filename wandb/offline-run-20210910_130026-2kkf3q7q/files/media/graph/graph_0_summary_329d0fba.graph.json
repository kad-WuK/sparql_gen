{"format": "torch", "nodes": [{"name": "transformer", "id": 2372093125128, "class_name": "GPTNeoModel(\n  (wte): Embedding(50258, 768)\n  (wpe): Embedding(2048, 768)\n  (drop): Dropout(p=0, inplace=False)\n  (h): ModuleList(\n    (0): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (1): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoLocalSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (2): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (3): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoLocalSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (4): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (5): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoLocalSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (6): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (7): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoLocalSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (8): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (9): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoLocalSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (10): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n    (11): GPTNeoBlock(\n      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (attn): GPTNeoAttention(\n        (attention): GPTNeoLocalSelfAttention(\n          (attn_dropout): Dropout(p=0, inplace=False)\n          (resid_dropout): Dropout(p=0, inplace=False)\n          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (mlp): GPTNeoMLP(\n        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n      )\n    )\n  )\n  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n)", "parameters": [["wte.weight", [50258, 768]], ["wpe.weight", [2048, 768]], ["h.0.ln_1.weight", [768]], ["h.0.ln_1.bias", [768]], ["h.0.attn.attention.k_proj.weight", [768, 768]], ["h.0.attn.attention.v_proj.weight", [768, 768]], ["h.0.attn.attention.q_proj.weight", [768, 768]], ["h.0.attn.attention.out_proj.weight", [768, 768]], ["h.0.attn.attention.out_proj.bias", [768]], ["h.0.ln_2.weight", [768]], ["h.0.ln_2.bias", [768]], ["h.0.mlp.c_fc.weight", [3072, 768]], ["h.0.mlp.c_fc.bias", [3072]], ["h.0.mlp.c_proj.weight", [768, 3072]], ["h.0.mlp.c_proj.bias", [768]], ["h.1.ln_1.weight", [768]], ["h.1.ln_1.bias", [768]], ["h.1.attn.attention.k_proj.weight", [768, 768]], ["h.1.attn.attention.v_proj.weight", [768, 768]], ["h.1.attn.attention.q_proj.weight", [768, 768]], ["h.1.attn.attention.out_proj.weight", [768, 768]], ["h.1.attn.attention.out_proj.bias", [768]], ["h.1.ln_2.weight", [768]], ["h.1.ln_2.bias", [768]], ["h.1.mlp.c_fc.weight", [3072, 768]], ["h.1.mlp.c_fc.bias", [3072]], ["h.1.mlp.c_proj.weight", [768, 3072]], ["h.1.mlp.c_proj.bias", [768]], ["h.2.ln_1.weight", [768]], ["h.2.ln_1.bias", [768]], ["h.2.attn.attention.k_proj.weight", [768, 768]], ["h.2.attn.attention.v_proj.weight", [768, 768]], ["h.2.attn.attention.q_proj.weight", [768, 768]], ["h.2.attn.attention.out_proj.weight", [768, 768]], ["h.2.attn.attention.out_proj.bias", [768]], ["h.2.ln_2.weight", [768]], ["h.2.ln_2.bias", [768]], ["h.2.mlp.c_fc.weight", [3072, 768]], ["h.2.mlp.c_fc.bias", [3072]], ["h.2.mlp.c_proj.weight", [768, 3072]], ["h.2.mlp.c_proj.bias", [768]], ["h.3.ln_1.weight", [768]], ["h.3.ln_1.bias", [768]], ["h.3.attn.attention.k_proj.weight", [768, 768]], ["h.3.attn.attention.v_proj.weight", [768, 768]], ["h.3.attn.attention.q_proj.weight", [768, 768]], ["h.3.attn.attention.out_proj.weight", [768, 768]], ["h.3.attn.attention.out_proj.bias", [768]], ["h.3.ln_2.weight", [768]], ["h.3.ln_2.bias", [768]], ["h.3.mlp.c_fc.weight", [3072, 768]], ["h.3.mlp.c_fc.bias", [3072]], ["h.3.mlp.c_proj.weight", [768, 3072]], ["h.3.mlp.c_proj.bias", [768]], ["h.4.ln_1.weight", [768]], ["h.4.ln_1.bias", [768]], ["h.4.attn.attention.k_proj.weight", [768, 768]], ["h.4.attn.attention.v_proj.weight", [768, 768]], ["h.4.attn.attention.q_proj.weight", [768, 768]], ["h.4.attn.attention.out_proj.weight", [768, 768]], ["h.4.attn.attention.out_proj.bias", [768]], ["h.4.ln_2.weight", [768]], ["h.4.ln_2.bias", [768]], ["h.4.mlp.c_fc.weight", [3072, 768]], ["h.4.mlp.c_fc.bias", [3072]], ["h.4.mlp.c_proj.weight", [768, 3072]], ["h.4.mlp.c_proj.bias", [768]], ["h.5.ln_1.weight", [768]], ["h.5.ln_1.bias", [768]], ["h.5.attn.attention.k_proj.weight", [768, 768]], ["h.5.attn.attention.v_proj.weight", [768, 768]], ["h.5.attn.attention.q_proj.weight", [768, 768]], ["h.5.attn.attention.out_proj.weight", [768, 768]], ["h.5.attn.attention.out_proj.bias", [768]], ["h.5.ln_2.weight", [768]], ["h.5.ln_2.bias", [768]], ["h.5.mlp.c_fc.weight", [3072, 768]], ["h.5.mlp.c_fc.bias", [3072]], ["h.5.mlp.c_proj.weight", [768, 3072]], ["h.5.mlp.c_proj.bias", [768]], ["h.6.ln_1.weight", [768]], ["h.6.ln_1.bias", [768]], ["h.6.attn.attention.k_proj.weight", [768, 768]], ["h.6.attn.attention.v_proj.weight", [768, 768]], ["h.6.attn.attention.q_proj.weight", [768, 768]], ["h.6.attn.attention.out_proj.weight", [768, 768]], ["h.6.attn.attention.out_proj.bias", [768]], ["h.6.ln_2.weight", [768]], ["h.6.ln_2.bias", [768]], ["h.6.mlp.c_fc.weight", [3072, 768]], ["h.6.mlp.c_fc.bias", [3072]], ["h.6.mlp.c_proj.weight", [768, 3072]], ["h.6.mlp.c_proj.bias", [768]], ["h.7.ln_1.weight", [768]], ["h.7.ln_1.bias", [768]], ["h.7.attn.attention.k_proj.weight", [768, 768]], ["h.7.attn.attention.v_proj.weight", [768, 768]], ["h.7.attn.attention.q_proj.weight", [768, 768]], ["h.7.attn.attention.out_proj.weight", [768, 768]], ["h.7.attn.attention.out_proj.bias", [768]], ["h.7.ln_2.weight", [768]], ["h.7.ln_2.bias", [768]], ["h.7.mlp.c_fc.weight", [3072, 768]], ["h.7.mlp.c_fc.bias", [3072]], ["h.7.mlp.c_proj.weight", [768, 3072]], ["h.7.mlp.c_proj.bias", [768]], ["h.8.ln_1.weight", [768]], ["h.8.ln_1.bias", [768]], ["h.8.attn.attention.k_proj.weight", [768, 768]], ["h.8.attn.attention.v_proj.weight", [768, 768]], ["h.8.attn.attention.q_proj.weight", [768, 768]], ["h.8.attn.attention.out_proj.weight", [768, 768]], ["h.8.attn.attention.out_proj.bias", [768]], ["h.8.ln_2.weight", [768]], ["h.8.ln_2.bias", [768]], ["h.8.mlp.c_fc.weight", [3072, 768]], ["h.8.mlp.c_fc.bias", [3072]], ["h.8.mlp.c_proj.weight", [768, 3072]], ["h.8.mlp.c_proj.bias", [768]], ["h.9.ln_1.weight", [768]], ["h.9.ln_1.bias", [768]], ["h.9.attn.attention.k_proj.weight", [768, 768]], ["h.9.attn.attention.v_proj.weight", [768, 768]], ["h.9.attn.attention.q_proj.weight", [768, 768]], ["h.9.attn.attention.out_proj.weight", [768, 768]], ["h.9.attn.attention.out_proj.bias", [768]], ["h.9.ln_2.weight", [768]], ["h.9.ln_2.bias", [768]], ["h.9.mlp.c_fc.weight", [3072, 768]], ["h.9.mlp.c_fc.bias", [3072]], ["h.9.mlp.c_proj.weight", [768, 3072]], ["h.9.mlp.c_proj.bias", [768]], ["h.10.ln_1.weight", [768]], ["h.10.ln_1.bias", [768]], ["h.10.attn.attention.k_proj.weight", [768, 768]], ["h.10.attn.attention.v_proj.weight", [768, 768]], ["h.10.attn.attention.q_proj.weight", [768, 768]], ["h.10.attn.attention.out_proj.weight", [768, 768]], ["h.10.attn.attention.out_proj.bias", [768]], ["h.10.ln_2.weight", [768]], ["h.10.ln_2.bias", [768]], ["h.10.mlp.c_fc.weight", [3072, 768]], ["h.10.mlp.c_fc.bias", [3072]], ["h.10.mlp.c_proj.weight", [768, 3072]], ["h.10.mlp.c_proj.bias", [768]], ["h.11.ln_1.weight", [768]], ["h.11.ln_1.bias", [768]], ["h.11.attn.attention.k_proj.weight", [768, 768]], ["h.11.attn.attention.v_proj.weight", [768, 768]], ["h.11.attn.attention.q_proj.weight", [768, 768]], ["h.11.attn.attention.out_proj.weight", [768, 768]], ["h.11.attn.attention.out_proj.bias", [768]], ["h.11.ln_2.weight", [768]], ["h.11.ln_2.bias", [768]], ["h.11.mlp.c_fc.weight", [3072, 768]], ["h.11.mlp.c_fc.bias", [3072]], ["h.11.mlp.c_proj.weight", [768, 3072]], ["h.11.mlp.c_proj.bias", [768]], ["ln_f.weight", [768]], ["ln_f.bias", [768]]], "output_shape": [[[[0], [0], [0], [0], [0], [0], [0], [0], 0, [0], [0], 0, 0, 0, 0, 0, 0], [[0], 0, 0, 0, 0, [0], 0, [0], 0, [0], 0, 0, [0], 0, 0]]], "num_parameters": [38598144, 1572864, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768, 589824, 589824, 589824, 589824, 768, 768, 768, 2359296, 3072, 2359296, 768, 768, 768]}, {"name": "lm_head", "id": 2372094114824, "class_name": "Linear(in_features=768, out_features=50258, bias=False)", "parameters": [["weight", [50258, 768]]], "output_shape": [[16, 218, 50258]], "num_parameters": [38598144]}], "edges": []}